{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "AI neural networks are inspired by neuroscience. In the brain, neurons\n",
    "are cells that are connected to each other, forming networks. Each\n",
    "neuron is capable of both receiving and sending electrical signals. Once\n",
    "the electrical input that a neuron receives crosses some threshold, the\n",
    "neuron activates, thus sending its electrical signal forward.\n",
    "\n",
    "An **Artificial Neural Network** is a mathematical model for learning\n",
    "inspired by biological neural networks. Artificial neural networks model\n",
    "mathematical functions that map inputs to outputs based on the structure\n",
    "and parameters of the network. In artificial neural networks, the\n",
    "structure of the network is shaped through training on data.\n",
    "\n",
    "When implemented in AI, the parallel of each neuron is a **unit** that’s\n",
    "connected to other units. For example, like in the last lecture, the AI\n",
    "might map two inputs, x₁ and x₂, to whether it is going to rain today or\n",
    "not. Last lecture, we suggested the following form for this hypothesis\n",
    "function: *h(x₁, x₂)* = *w₀ + w₁x₁ + w₂x₂*, where *w₁* and *w₂* are\n",
    "weights that modify the inputs, and *w₀* is a constant, also called\n",
    "**bias**, modifying the value of the whole expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "To use the hypothesis function to decide whether it rains or not, we\n",
    "need to create some sort of threshold based on the value it produces.\n",
    "\n",
    "One way to do this is with a step function, which gives 0 before a\n",
    "certain threshold is reached and 1 after the threshold is reached.\n",
    "\n",
    "![Step Function](https://cs50.harvard.edu/ai/2020/notes/5/step.png)\n",
    "\n",
    "Another way to go about this is with a logistic function, which gives as\n",
    "output any real number from 0 to 1, thus expressing graded confidence in\n",
    "its judgment.\n",
    "\n",
    "![Logistic Function](https://cs50.harvard.edu/ai/2020/notes/5/logistic.png)\n",
    "\n",
    "Another possible function is Rectified Linear Unit (ReLU), which allows\n",
    "the output to be any positive value. If the value is negative, ReLU sets\n",
    "it to 0.\n",
    "\n",
    "![Rectified Linear Unit](https://cs50.harvard.edu/ai/2020/notes/5/relu.png)\n",
    "\n",
    "Whichever function we choose to use, we learned last lecture that the\n",
    "inputs are modified by weights in addition to the bias, and the sum of\n",
    "those is passed to an activation function. This stays true for simple\n",
    "neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Structure\n",
    "\n",
    "A neural network can be thought of as a representation of the idea\n",
    "above, where a function sums up inputs to produce an output.\n",
    "\n",
    "![Neural Network Structure](https://cs50.harvard.edu/ai/2020/notes/5/nnstructure.png)\n",
    "\n",
    "The two white units on the left are the input and the unit on the right\n",
    "is an output. The inputs are connected to the output by a weighted edge.\n",
    "To make a decision, the output unit multiplies the inputs by their\n",
    "weights in addition to the bias (*w₀*), and the uses function *g* to\n",
    "determine the output.\n",
    "\n",
    "For example, an Or logical connective can be represented as a function\n",
    "*f* with the following truth table:\n",
    "\n",
    "<span class=\"loading-wrap\"> <span class=\"loading-text\">Loading, please\n",
    "wait</span>\n",
    "<span class=\"animation-wrap\"><span class=\"animation-dot\"></span></span>\n",
    "</span>\n",
    "\n",
    "<table class=\"table table-bordered table-striped\">\n",
    "<colgroup>\n",
    "<col style=\"width: 33%\" />\n",
    "<col style=\"width: 33%\" />\n",
    "<col style=\"width: 33%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr>\n",
    "<th data-field=\"0\"><div class=\"th-inner\">\n",
    "<div style=\"text-align: left\">\n",
    "<em>x</em>\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"fht-cell\">\n",
    "&#10;</div></th>\n",
    "<th data-field=\"1\"><div class=\"th-inner\">\n",
    "<div style=\"text-align: left\">\n",
    "<em>y</em>\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"fht-cell\">\n",
    "&#10;</div></th>\n",
    "<th data-field=\"2\"><div class=\"th-inner\">\n",
    "<div style=\"text-align: left\">\n",
    "<em>f(x, y)</em>\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"fht-cell\">\n",
    "&#10;</div></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr data-index=\"0\">\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "<tr data-index=\"1\">\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "<tr data-index=\"2\">\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "<tr data-index=\"3\">\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "We can visualize this function as a neural network. *x₁* is one input\n",
    "unit, and *x₂* is another input unit. They are connected to the output\n",
    "unit by an edge with a weight of 1. The output unit then uses function\n",
    "*g(-1 + 1x₁ + 2x₂)* with a threshold of 0 to output either 0 or 1 (false\n",
    "or true).\n",
    "\n",
    "![Neural Network of Or Function](https://cs50.harvard.edu/ai/2020/notes/5/nnor.png)\n",
    "\n",
    "For example, in the case where *x₁* = *x₂* = 0, the sum is (-1). This is\n",
    "below the threshold, so the function *g* will output 0. However, if\n",
    "either or both of *x₁* or *x₂* are equal to 1, then the sum of all\n",
    "inputs will be either 0 or 1. Both are at or above the threshold, so the\n",
    "function will output 1.\n",
    "\n",
    "A similar process can be repeated with the And function (where the bias\n",
    "will be (-2)). Moreover, inputs and outputs don’t have to be distinct. A\n",
    "similar process can be used to take humidity and air pressure as input,\n",
    "and produce the probability of rain as output. Or, in a different\n",
    "example, inputs can be money spent on advertising and the month when it\n",
    "was spent to get the output of expected revenue from sales. This can be\n",
    "extended to any number of inputs by multiplying each input *x₁ … xₙ* by\n",
    "weight *w₁ … wₙ*, summing up the resulting values and adding a bias\n",
    "*w₀*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent is an algorithm for minimizing loss when training\n",
    "neural networks. As was mentioned earlier, a neural network is capable\n",
    "of inferring knowledge about the structure of the network itself from\n",
    "the data. Whereas, so far, we defined the different weights, neural\n",
    "networks allow us to compute these weights based on the training data.\n",
    "To do this, we use the gradient descent algorithm, which works the\n",
    "following way:\n",
    "\n",
    "- Start with a random choice of weights.\n",
    "    This is our naive starting place, where we don’t know how much we\n",
    "    should weight each input.\n",
    "- Repeat:\n",
    "  - Calculate the gradient based on all\n",
    "        data points that will lead to decreasing loss. Ultimately, the\n",
    "        gradient is a vector (a sequence of numbers).\n",
    "  - Update weights according to the\n",
    "        gradient.\n",
    "\n",
    "The problem with this kind of algorithm is that it requires to calculate\n",
    "the gradient based on *all data points*, which is computationally\n",
    "costly. There are a multiple ways to minimize this cost. For example, in\n",
    "**Stochastic Gradient Descent**, the gradient is calculated based on one\n",
    "point chosen at random. This kind of gradient can be quite inaccurate,\n",
    "leading to the **Mini-Batch Gradient Descent** algorithm, which computes\n",
    "the gradient based on on a few points selected at random, thus finding a\n",
    "compromise between computation cost and accuracy. As often is the case,\n",
    "none of these solutions is perfect, and different solutions might be\n",
    "employed in different situations.\n",
    "\n",
    "Using gradient descent, it is possible to find answers to many problems.\n",
    "For example, we might want to know more than “will it rain today?” We\n",
    "can use some inputs to generate probabilities for different kinds of\n",
    "weather, and then just choose the weather that is most probable.\n",
    "\n",
    "![Neural Network for Weather](https://cs50.harvard.edu/ai/2020/notes/5/weather.png)\n",
    "\n",
    "This can be done with any number of inputs and outputs, where each input\n",
    "is connected to each output, and where the outputs represent decisions\n",
    "that we can make. Note that in this kind of neural networks the outputs\n",
    "are not connected. This means that each output and its associated\n",
    "weights from all the inputs can be be seen as an individual neural\n",
    "network and thus can be trained separately from the rest of the outputs.\n",
    "\n",
    "So far, our neural networks relied on **perceptron** output units. These\n",
    "are units that are only capable of learning a linear decision boundary,\n",
    "using a straight line to separate data. That is, based on a linear\n",
    "equation, the perceptron could classify an input to be one type or\n",
    "another (e.g. left picture). However, often, data are not linearly\n",
    "separable (e.g. right picture). In this case, we turn to multilayer\n",
    "neural networks to model data non-linearly.\n",
    "\n",
    "![Linear and Non-Linear Models](https://cs50.harvard.edu/ai/2020/notes/5/linearnonlinear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Neural Networks\n",
    "\n",
    "A multilayer neural network is an artificial neural network with an\n",
    "input layer, an output layer, and at least one **hidden** layer. While\n",
    "we provide inputs and outputs to train the model, we, the humans, don’t\n",
    "provide any values to the units inside the hidden layers. Each unit in\n",
    "the first hidden layer receives a weighted value from each of the units\n",
    "in the input layer, performs some action on it and outputs a value. Each\n",
    "of these values is weighted and further propagated to the next layer,\n",
    "repeating the process until the output layer is reached. Through hidden\n",
    "layers, it is possible to model non-linear data.\n",
    "\n",
    "![Multilayer Neural Network](https://cs50.harvard.edu/ai/2020/notes/5/multilayer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Backpropagation is the main algorithm used for training neural networks\n",
    "with hidden layers. It does so by starting with the errors in the output\n",
    "units, calculating the gradient descent for the weights of the previous\n",
    "layer, and repeating the process until the input layer is reached. In\n",
    "pseudocode, we can describe the algorithm as follows:\n",
    "\n",
    "- Calculate error for output layer\n",
    "- For each layer, starting with output\n",
    "    layer and moving inwards towards earliest hidden layer:\n",
    "  - Propagate error back one layer. In\n",
    "        other words, the current layer that’s being considered sends the\n",
    "        errors to the preceding layer.\n",
    "  - Update weights.\n",
    "\n",
    "This can be extended to any number of hidden layers, creating **deep\n",
    "neural networks**, which are neural networks that have more than one\n",
    "hidden layer.\n",
    "\n",
    "![Deep Neural Network](https://cs50.harvard.edu/ai/2020/notes/5/deepnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "Overfitting is the danger of modeling the training data too closely,\n",
    "thus failing to generalize to new data. One way to combat overfitting is\n",
    "by **dropout**. In this technique, we temporarily remove units that we\n",
    "select at random during the learning phase. This way, we try to prevent\n",
    "over-reliance on any one unit in the network. Throughout training, the\n",
    "neural network will assume different forms, each time dropping some\n",
    "other units and then using them again:\n",
    "\n",
    "![Dropout](https://cs50.harvard.edu/ai/2020/notes/5/dropout.png)\n",
    "\n",
    "Note that after the training is finished, the whole neural network will\n",
    "be used again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "Like often is the case in python, multiple libraries already have an\n",
    "implementation for neural networks using the backpropagation algorithm,\n",
    "and TensorFlow is one such library. You are welcome to experiment with\n",
    "TensorFlow neural networks in this [web\n",
    "application](http://playground.tensorflow.org/), which lets you define\n",
    "different properties of the network and run it, visualizing the output.\n",
    "We will now turn to an example of how we can use TensorFlow to perform\n",
    "the task we discussed last lecture: distinguishing counterfeit notes\n",
    "from genuine notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import TensorFlow and call it tf (to make the code shorter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in from file\n",
    "with open(\"banknotes/banknotes.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "\n",
    "    data = []\n",
    "    for row in reader:\n",
    "        data.append({\n",
    "            \"evidence\": [float(cell) for cell in row[:4]],\n",
    "            \"label\": 1 if row[4] == \"0\" else 0\n",
    "        })\n",
    "\n",
    "# Separate data into training and testing groups\n",
    "evidence = [row[\"evidence\"] for row in data]\n",
    "labels = [row[\"label\"] for row in data]\n",
    "X_training, X_testing, y_training, y_testing = train_test_split(\n",
    "    evidence, labels, test_size=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the CSV data to the model. Our work is often required in\n",
    "making the data fit the format that the library requires. The difficult\n",
    "part of actually coding the model is already implemented for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network\n",
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is an api that different machine learning algorithms access. A\n",
    "sequential model is one where layers follow each other (like the ones we\n",
    "have seen so far)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a hidden layer with 8 units, with ReLU activation\n",
    "model.add(tf.keras.layers.Dense(8, input_shape=(4,), activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dense layer is one where each node in the current layer is connected\n",
    "to all the nodes from the previous layer. In generating our hidden\n",
    "layers we create 8 dense layers, each having 4 input neurons, using the\n",
    "ReLU activation function mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add output layer with 1 unit, with sigmoid activation\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our output layer, we want to create one dense layer that uses a\n",
    "sigmoid activation function, an activation function where the output is\n",
    "a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "26/26 [==============================] - 0s 649us/step - loss: 1.3290 - accuracy: 0.2017\n",
      "Epoch 2/20\n",
      "26/26 [==============================] - 0s 589us/step - loss: 1.0877 - accuracy: 0.2394\n",
      "Epoch 3/20\n",
      "26/26 [==============================] - 0s 562us/step - loss: 0.9076 - accuracy: 0.3171\n",
      "Epoch 4/20\n",
      "26/26 [==============================] - 0s 536us/step - loss: 0.7893 - accuracy: 0.4459\n",
      "Epoch 5/20\n",
      "26/26 [==============================] - 0s 514us/step - loss: 0.7058 - accuracy: 0.5456\n",
      "Epoch 6/20\n",
      "26/26 [==============================] - 0s 538us/step - loss: 0.6342 - accuracy: 0.6002\n",
      "Epoch 7/20\n",
      "26/26 [==============================] - 0s 521us/step - loss: 0.5685 - accuracy: 0.6501\n",
      "Epoch 8/20\n",
      "26/26 [==============================] - 0s 525us/step - loss: 0.5074 - accuracy: 0.6731\n",
      "Epoch 9/20\n",
      "26/26 [==============================] - 0s 512us/step - loss: 0.4528 - accuracy: 0.7096\n",
      "Epoch 10/20\n",
      "26/26 [==============================] - 0s 521us/step - loss: 0.4060 - accuracy: 0.7655\n",
      "Epoch 11/20\n",
      "26/26 [==============================] - 0s 806us/step - loss: 0.3667 - accuracy: 0.8214\n",
      "Epoch 12/20\n",
      "26/26 [==============================] - 0s 637us/step - loss: 0.3341 - accuracy: 0.8542\n",
      "Epoch 13/20\n",
      "26/26 [==============================] - 0s 671us/step - loss: 0.3055 - accuracy: 0.8834\n",
      "Epoch 14/20\n",
      "26/26 [==============================] - 0s 714us/step - loss: 0.2808 - accuracy: 0.8943\n",
      "Epoch 15/20\n",
      "26/26 [==============================] - 0s 585us/step - loss: 0.2597 - accuracy: 0.9174\n",
      "Epoch 16/20\n",
      "26/26 [==============================] - 0s 598us/step - loss: 0.2420 - accuracy: 0.9368\n",
      "Epoch 17/20\n",
      "26/26 [==============================] - 0s 569us/step - loss: 0.2257 - accuracy: 0.9465\n",
      "Epoch 18/20\n",
      "26/26 [==============================] - 0s 546us/step - loss: 0.2113 - accuracy: 0.9526\n",
      "Epoch 19/20\n",
      "26/26 [==============================] - 0s 525us/step - loss: 0.1987 - accuracy: 0.9575\n",
      "Epoch 20/20\n",
      "26/26 [==============================] - 0s 498us/step - loss: 0.1872 - accuracy: 0.9611\n",
      "18/18 - 0s - loss: 0.1878 - accuracy: 0.9581 - 64ms/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18779660761356354, 0.9581056237220764]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train neural network\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(X_training, y_training, epochs=20)\n",
    "\n",
    "# Evaluate how well model performs\n",
    "model.evaluate(X_testing, y_testing, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compile the model, specifying which algorithm should\n",
    "optimize it, what type of loss function we use, and how we want to\n",
    "measure its success (in our case, we are interested in the accuracy of\n",
    "the output). Finally, we fit the model on the training data with 20\n",
    "repetitions (epochs), and then evaluate it on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision\n",
    "\n",
    "Computer vision encompasses the different computational methods for\n",
    "analyzing and understanding digital images, and it is often achieved\n",
    "using neural networks. For example, computer vision is used when social\n",
    "media employs face recognition to automatically tag people in pictures.\n",
    "Other examples are handwriting recognition and self-driving cars.\n",
    "\n",
    "Images consist of pixels, and pixels are represented by three values\n",
    "that range from 0 to 255, one for red, one for green and one for blue.\n",
    "These values are often referred to with the acronym RGB. We can use this\n",
    "to create a neural network where each color value in each pixel is an\n",
    "input, where we have some hidden layers, and the output is some number\n",
    "of units that tell us what it is that was shown in the image. However,\n",
    "there are a few drawbacks to this approach. First, by breaking down the\n",
    "image into pixels and the values of their colors, we can’t use the\n",
    "structure of the image as an aid. That is, as humans, if we see a part\n",
    "of a face we know to expect to see the rest of the face, and this\n",
    "quickens computation. We want to be able to use a similar advantage in\n",
    "our neural networks. Second, the sheer number of inputs is very big,\n",
    "which means that we will have to calculate a lot of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Convolution\n",
    "\n",
    "Image convolution is applying a filter that adds each pixel value of an\n",
    "image to its neighbors, weighted according to a kernel matrix. Doing so\n",
    "alters the image and can help the neural network process it.\n",
    "\n",
    "Let’s consider the following example:\n",
    "\n",
    "![Image Convolution](https://cs50.harvard.edu/ai/2020/notes/5/convolution.png)\n",
    "\n",
    "The kernel is the blue matrix, and the image is the big matrix on the\n",
    "left. The resulting filtered image is the small matrix on the bottom\n",
    "right. To filter the image with the kernel, we start with the pixel with\n",
    "value 20 in the top-left of the image (coordinates 1,1). Then, we will\n",
    "multiply all the values around it by the corresponding value in the\n",
    "kernel and sum them up (10\\*0 + 20\\*(-1) + 30\\*0 + 10\\*(-1) + 20\\*5 +\n",
    "30\\*(-1) + 20\\*0 + 30\\*(-1) + 40\\*0), producing the value 10. Then we\n",
    "will do the same for the pixel on the right (30), the pixel below the\n",
    "first one (30), and the pixel to the right of this one (40). This\n",
    "produces a filtered image with the values we see on the bottom right.\n",
    "\n",
    "Different kernels can achieve different tasks. For edge detection, the\n",
    "following kernel is often used:\n",
    "\n",
    "![Edge Detection Kernel](https://cs50.harvard.edu/ai/2020/notes/5/kernel.png)\n",
    "\n",
    "The idea here is that when the pixel is similar to all its neighbors,\n",
    "they should cancel each other, giving a value of 0. Therefore, the more\n",
    "similar the pixels, the darker the part of the image, and the more\n",
    "different they are the lighter it is. Applying this kernel to an image\n",
    "(left) results in an image with pronounced edges (right):\n",
    "\n",
    "![Edge Detection](https://cs50.harvard.edu/ai/2020/notes/5/edgedetection.png)\n",
    "\n",
    "Let’s consider an implementation of image convolution. We are using the\n",
    "PIL library (stands for Python Imaging Library) that can do most of the\n",
    "hard work for us.\n",
    "\n",
    "```python\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "# Ensure correct usage\n",
    "if len(sys.argv) != 2:\n",
    "    sys.exit(\"Usage: python filter.py filename\")\n",
    "\n",
    "# Open image\n",
    "image = Image.open(sys.argv[1]).convert(\"RGB\")\n",
    "\n",
    "# Filter image according to edge detection kernel\n",
    "filtered = image.filter(ImageFilter.Kernel(\n",
    "    size=(3, 3),\n",
    "    kernel=[-1, -1, -1, -1, 8, -1, -1, -1, -1],\n",
    "    scale=1\n",
    "))\n",
    "\n",
    "# Show resulting image\n",
    "filtered.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure(640x480)\n"
     ]
    }
   ],
   "source": [
    "!python convolution/filter.py convolution/bridge.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, processing the image in a neural network is computationally\n",
    "expensive due to the number of pixels that serve as input to the neural\n",
    "network. Another way to go about this is **Pooling**, where the size of\n",
    "the input is reduced by sampling from regions in the input. Pixels that\n",
    "are next to each other belong to the same area in the image, which means\n",
    "that they are likely to be similar. Therefore, we can take one pixel to\n",
    "represent a whole area. One way of doing this is with **Max-Pooling**,\n",
    "where the selected pixel is the one with the highest value of all others\n",
    "in the same region. For example, if we divide the left square (below)\n",
    "into four 2X2 squares, by max-pooling from this input, we get the small\n",
    "square on the right.\n",
    "\n",
    "![Max-Pooling](https://cs50.harvard.edu/ai/2020/notes/5/maxpooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "A convolutional neural network is a neural network that uses\n",
    "convolution, usually for analyzing images. It starts by applying filters\n",
    "that can help distill some features of the image using different\n",
    "kernels. These filters can be improved in the same way as other weights\n",
    "in the neural network, by adjusting their kernels based on the error of\n",
    "the output. Then, the resulting images are pooled, after which the\n",
    "pixels are fed to a traditional neural network as inputs (a process\n",
    "called **flattening**).\n",
    "\n",
    "![Convolutional Neural Network](https://cs50.harvard.edu/ai/2020/notes/5/convolutionalnn.png)\n",
    "\n",
    "The convolution and pooling steps can be repeated multiple times to\n",
    "extract additional features and reduce the size of the input to the\n",
    "neural network. One of the benefits of these processes is that, by\n",
    "convoluting and pooling, the neural network becomes less sensitive to\n",
    "variation. That is, if the same picture is taken from slightly different\n",
    "angles, the input for convolutional neural network will be similar,\n",
    "whereas, without convolution and pooling, the input from each image\n",
    "would be vastly different.\n",
    "\n",
    "In code, a convolutional neural network doesn’t differ by much from a\n",
    "traditional neural network. TensorFlow offers datasets to test our\n",
    "models on. We will be using MNIST, which contains pictures of black and\n",
    "white handwritten digits. We will train our convolutional neural network\n",
    "to recognize digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 08:31:01.935571: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2613 - accuracy: 0.9216\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1078 - accuracy: 0.9679\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0832 - accuracy: 0.9748\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 8s 5ms/step - loss: 0.0663 - accuracy: 0.9795\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0579 - accuracy: 0.9823\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0501 - accuracy: 0.9845\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0430 - accuracy: 0.9865\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0382 - accuracy: 0.9871\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0343 - accuracy: 0.9887\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0312 - accuracy: 0.9900\n",
      "313/313 - 0s - loss: 0.0389 - accuracy: 0.9895 - 413ms/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03893917053937912, 0.9894999861717224]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Use MNIST handwriting dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# Prepare data for training\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "x_train = x_train.reshape(\n",
    "    x_train.shape[0], x_train.shape[1], x_train.shape[2], 1\n",
    ")\n",
    "x_test = x_test.reshape(\n",
    "    x_test.shape[0], x_test.shape[1], x_test.shape[2], 1\n",
    ")\n",
    "\n",
    "# Create a convolutional neural network\n",
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    # Convolutional layer. Learn 32 filters using a 3x3 kernel\n",
    "    tf.keras.layers.Conv2D(\n",
    "        32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)\n",
    "    ),\n",
    "\n",
    "    # Max-pooling layer, using 2x2 pool size\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Flatten units\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # Add a hidden layer with dropout\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    # Add an output layer with output units for all 10 digits\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Train neural network\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "# Evaluate neural network performance\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model takes time to train, we can save the already trained\n",
    "model to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: --f=/home/vscode/.local/share/jupyter/runtime/kernel-v2-445L097fdWo5YpA.json/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: --f=/home/vscode/.local/share/jupyter/runtime/kernel-v2-445L097fdWo5YpA.json/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to --f=/home/vscode/.local/share/jupyter/runtime/kernel-v2-445L097fdWo5YpA.json.\n"
     ]
    }
   ],
   "source": [
    "# Save model to file\n",
    "if len(sys.argv) == 2:\n",
    "    filename = sys.argv[1]\n",
    "    model.save(filename)\n",
    "    print(f\"Model saved to {filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we run a program that receives hand-drawn digits as input, it\n",
    "will be able to classify and output the digit using the model. For an\n",
    "implementation of such a program, refer to recognition.py in the source\n",
    "code for this lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "**Feed-Forward Neural Networks** are the type of neural networks that we\n",
    "have discussed so far, where input data is provided to the network,\n",
    "which eventually produces some output. A diagram of how feed-forward\n",
    "neural networks work can be seen below.\n",
    "\n",
    "![Feed-Forward Neural Networks Diagram](https://cs50.harvard.edu/ai/2020/notes/5/ffnndiagram.png)\n",
    "\n",
    "As opposed to that, **Recurrent Neural Networks** consist of a\n",
    "non-linear structure, where the network uses its own output as input.\n",
    "For example, Microsoft’s [captionbot](https://www.captionbot.ai) is\n",
    "capable of describing the content of an image with words in a sentence.\n",
    "This is different from classification in that the output can be of\n",
    "varying length based on the properties of the image. While feed-forward\n",
    "neural networks are incapable of varying the number of outputs,\n",
    "recurrent neural networks are capable to do that due to their structure.\n",
    "In the captioning task, a network would process the input to produce an\n",
    "output, and then continue processing from that point on, producing\n",
    "another output, and repeating as much as necessary.\n",
    "\n",
    "![Recurrent Neural Network](https://cs50.harvard.edu/ai/2020/notes/5/recurrent.png)\n",
    "\n",
    "Recurrent neural networks are helpful in cases where the network deals\n",
    "with sequences and not a single individual object. Above, the neural\n",
    "network needed to produce a sequence of words. However, the same\n",
    "principle can be applied to analyzing video files, which consist of a\n",
    "sequence of images, or in translation tasks, where a sequence of inputs\n",
    "(words in the source language) is processed to produce a sequence of\n",
    "outputs (words in the target language)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
